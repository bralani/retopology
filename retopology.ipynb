{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d, os\n",
    "import ctypes\n",
    "from ctypes import *\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Since our dataset is quite big and it take several hours to preprocess it, we have uploaded the preprocessed dataset in a zip file. You can download it from [here](https://drive.google.com).\n",
    "Be sure to extract the zip file and upload the dataset to the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Skip this section if you have downloaded the preprocessed dataset and go directly to the training of neural network.\n",
    "\n",
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to augment the data in order to prevent overfitting.\n",
    "First step is to rotate and scale (without changing the aspect ratio) the meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rotate_scale\n",
    "\n",
    "files = glob(\"./new_dataset/train/input/quads/*\")\n",
    "for file in files:\n",
    "    rotate_scale(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we generate the trimesh objects from the quads meshes. This is done because the input of our network must be a trimesh object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_trimesh\n",
    "\n",
    "files = glob(\"./new_dataset/train/input/quads/*\")\n",
    "\n",
    "for file in files:\n",
    "    generate_trimesh(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastyl, we generate the output (true lables) of the network, which are the direction fields of the vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_output\n",
    "import numpy as np\n",
    "\n",
    "files_quads = glob(\"./new_dataset/train/input/quads/*\")\n",
    "\n",
    "for file in files_quads:\n",
    "  file_name = os.path.basename(file)\n",
    "  file_triangle = \"./new_dataset/train/input/triangles/\" + file_name\n",
    "\n",
    "  orientation_fields = generate_output(file_triangle, file)\n",
    "  orientation_fields_reshaped = orientation_fields.reshape(orientation_fields.shape[0], -1)\n",
    "\n",
    "\n",
    "  # save the orientation fields in a txt file\n",
    "  output_file = \"./new_dataset/train/output/\" + file_name\n",
    "  np.savetxt(output_file, orientation_fields_reshaped)\n",
    "\n",
    "\n",
    "  print(\"Orientation fields saved in: \", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading mesh to retopologize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No shared library found, please build the shared library first (README.md for instructions)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading the shared library with C++\u001b[39;00m\n\u001b[0;32m      2\u001b[0m shared_library_path \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./build\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.so\u001b[39m\u001b[38;5;124m\"\u001b[39m), recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m shared_library_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo shared library found, please build the shared library first (README.md for instructions)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m shared_library \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(shared_library_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./new_dataset/train/input/triangles\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No shared library found, please build the shared library first (README.md for instructions)"
     ]
    }
   ],
   "source": [
    "# loading the shared library with C++\n",
    "shared_library_path = glob(os.path.join(\"./build\", \"**/*.so\"), recursive=True)\n",
    "assert shared_library_path, \"No shared library found, please build the shared library first (README.md for instructions)\"\n",
    "shared_library = ctypes.CDLL(shared_library_path[0])\n",
    "\n",
    "file_path = \"./new_dataset/train/input/triangles\"\n",
    "file_output_path = os.path.join(file_path, \"..\", \"..\", \"output\")\n",
    "\n",
    "files_triangles = glob(\"./new_dataset/train/input/triangles/*\")\n",
    "\n",
    "for file_triangle in files_triangles:\n",
    "\n",
    "    mesh = open3d.io.read_triangle_mesh(file_triangle)\n",
    "\n",
    "    vertices = mesh.vertices\n",
    "    triangles = mesh.triangles\n",
    "\n",
    "    num_vertices = len(vertices)\n",
    "    num_triangles = len(triangles)\n",
    "\n",
    "    # create file input.txt\n",
    "    file = open(\"input.txt\", \"w\")\n",
    "\n",
    "    # write vertices\n",
    "    for i in range(len(vertices)):\n",
    "        file.write(str(vertices[i][0]) + \" \" + str(vertices[i][1]) + \" \" + str(vertices[i][2]) + \"\\n\")\n",
    "\n",
    "    # write triangles\n",
    "    for i in range(len(triangles)):\n",
    "        file.write(str(triangles[i][0]) + \" \" + str(triangles[i][1]) + \" \" + str(triangles[i][2]) + \"\\n\")\n",
    "\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    k = 3 # neighborhood size\n",
    "\n",
    "    file_output = os.path.join(file_output_path, os.path.basename(file_triangle))\n",
    "\n",
    "    # get the complete url of the file\n",
    "    file_output = os.path.abspath(file_output)\n",
    "\n",
    "    # modify the basename concatenating _principal_directions\n",
    "    file_output = file_output.replace(\".obj\", \"_principal_directions.txt\")\n",
    "\n",
    "    output = create_string_buffer(file_output.encode('utf-8'),size=100)\n",
    "    shared_library.crest_lines(num_vertices, num_triangles, k, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA...\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU...\")\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "# Problem/dataset things\n",
    "n_class = 4\n",
    "\n",
    "# Model \n",
    "input_features = 'hks'      # 'hks' or 'xyz'\n",
    "k_eig = 128\n",
    "\n",
    "# Training settings\n",
    "train = True                # Whether to train the model\n",
    "n_epoch = 10000\n",
    "lr = 1e-4\n",
    "decay_every = 50\n",
    "decay_rate = 0.5\n",
    "augment_random_rotate = (input_features == 'xyz')\n",
    "\n",
    "# Important paths\n",
    "base_path = Path(os.getcwd())\n",
    "op_cache_dir = os.path.join(base_path, \"neural_network\", \"data\", \"op_cache\")\n",
    "dataset_path = os.path.join(base_path, \"new_dataset\")\n",
    "dataset_path = os.path.normpath(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from neural_network.dataset import MeshDataset\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = MeshDataset(dataset_path, train=False, k_eig=k_eig, use_cache=False, op_cache_dir=op_cache_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=None)\n",
    "\n",
    "# Load the train dataset\n",
    "if train:\n",
    "    train_dataset = MeshDataset(dataset_path, train=True, k_eig=k_eig, use_cache=False, op_cache_dir=op_cache_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import diffusion_net\n",
    "\n",
    "# === Create the model\n",
    "\n",
    "C_in={'xyz':3, 'hks':12}[input_features] # Dimension of input features\n",
    "\n",
    "model = diffusion_net.layers.DiffusionNet(C_in=C_in,\n",
    "                                          C_out=n_class,\n",
    "                                          C_width=128, \n",
    "                                          N_block=5,\n",
    "                                          outputs_at='vertices', \n",
    "                                          dropout=True)\n",
    "\n",
    "if os.path.exists('saved_model.pth'):\n",
    "    model.load_state_dict(torch.load('saved_model.pth', map_location=device))\n",
    "    print('Loaded model from: saved_model.pth')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# === Optimize\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network.neural_network_management import train_epoch, test\n",
    "\n",
    "if train:\n",
    "    print(\"Training...\")\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            print('Model saved in: saved_model.pth')\n",
    "        train_loss = train_epoch(epoch, decay_every, decay_rate, optimizer, model, train_loader, device, augment_random_rotate, input_features)\n",
    "        test_loss = test(model, test_loader, device, input_features)\n",
    "        print(\"Epoch {} - Train overall: {:06.3f}  Test overall: {:06.3f}\".format(epoch, train_loss, test_loss))\n",
    "\n",
    "\n",
    "test_loss = test()\n",
    "print(\"Overall test loss: {:06.3f}%\".format(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
